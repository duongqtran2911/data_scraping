import pandas as pd
import datetime
from datetime import datetime, timezone
from pymongo import MongoClient
import uuid
from datetime import datetime
import numpy as np
import json
from write_data_utils import normalize_att, find_row_index_containing, smart_parse_float, \
        find_comparison_table_start, get_land_price_raw, get_land_price_pct, get_info_location, get_info_purpose, \
        get_info_unit_price, find_meta_data, find_comparison_table_end, find_raw_table_end, match_idx, parse_human_number
import os
import re
from pymongo import MongoClient
import traceback
import logging



# ---- CONFIGURATION ----
# EXCEL_FILE = "DV_Can Giuoc.xlsx"
# EXCEL_FILE = "DV_Le Trong Tan.xlsx"
MONGO_URI = "mongodb://dev-valuemind:W57mFPVT57lt3wU@10.10.0.42:27021/?replicaSet=rs0&directConnection=true&authSource=assets-valuemind"


# ---- GLOBAL PARAMETERS ----
sheet_idx = 0
raw_col_length = 11
pct_col_length = 6
year = 2024
month = "8"

# Read list of Excel paths
# with open(f"comparison_files_{month}_{year}.txt", "r", encoding="utf-8") as f:
#     excel_paths = [line.strip() for line in f if line.strip()]
# open(log_file_path", "w", encoding="utf-8").close()
# total_sheets = 0


# ---- LOAD EXCEL PATHS AND SHEETS ----
# comparison_file_path = f"comparison_files_{month}_{year}.txt"
# sheet_map = {}
#
# detect_folder = os.path.join(os.getcwd(), f"file_detection_{year}")
# comparison_file = os.path.join(detect_folder, comparison_file_path)
#
# with open(comparison_file, "r", encoding="utf-8") as f:
#     for line in f:
#         if ">>>" in line:
#             path, sheets = line.rstrip("\n").split(" >>> ")
#             sheet_map[path.strip()] = [s for s in sheets.split("&&")]

folder_path = r"D:/excel"
sheet_map = {}

# QuÃ©t toÃ n bá»™ file Excel trong thÆ° má»¥c
for filename in os.listdir(folder_path):
    if filename.endswith(".xlsx") or filename.endswith(".xls"):
        full_path = os.path.join(folder_path, filename)
        try:
            xls = pd.ExcelFile(full_path)
            sheet_map[full_path] = xls.sheet_names  # Láº¥y danh sÃ¡ch sheet
        except Exception as e:
            print(f"âš ï¸ Lá»—i Ä‘á»c file {full_path}: {e}")

# ---- LOG FILE ----
log_folder = os.path.join(os.getcwd(), f"reading_logs_{year}")
os.makedirs(log_folder, exist_ok=True)


# ğŸ“„ Create log file path inside the year-based folder
# log_file_path = f"reading_logs_{month}_{year}.txt"
log_file_path = os.path.join(log_folder, f"reading_logs_{month}_{year}.txt")
open(log_file_path, "w", encoding="utf-8").close()

found = 0
missing = 0

# ---- PROCESS EACH FILE AND SHEET ----
for file_path, sheet_list in sheet_map.items():
    try:
        if not os.path.isfile(file_path):
            print(f"â­•ï¸ File not found: {file_path}")
            missing += 1
            with open(log_file_path, "a", encoding="utf-8") as log_file:
                log_file.write(f"â­•ï¸ File not found: {file_path}\n")
                log_file.write("----------------------------------------------------------------------------------------------\n")
            continue
        
        with open(log_file_path, "a", encoding="utf-8") as log_file:
            log_file.write(f"sheet_list: {sheet_list}\n")
        xls = pd.ExcelFile(file_path)
        sheet_names = xls.sheet_names
        with open(log_file_path, "a", encoding="utf-8") as log_file:
            log_file.write(f"sheet_names: {sheet_names}\n")

        for sheet in sheet_list:
            try: 
                if sheet not in xls.sheet_names:
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"âŒ Sheet '{sheet}' not found in {file_path}\n")
                        log_file.write("----------------------------------------------------------------------------------------------\n") 
                    continue

                # --- Insert your full parsing/writing logic here ---
                print(f"â³ Processing {file_path}")
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"â³ Processing sheet: {sheet}, {file_path}\n")

                # xls = pd.ExcelFile(file_path)
                # sheet = xls.sheet_names[sheet_idx]

                
                df = pd.read_excel(xls, sheet_name=sheet, header=None)
                df = df.dropna(axis=1, how='all')
                min_valid_cells = 5
                df = df.loc[:, df.notna().sum() >= min_valid_cells].reset_index(drop=True)

                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"df:\n {df}")

                

                # ---- DETECT RAW DATA TABLE AND COMPARISON TABLE ---- 
                raw_start_idx = find_row_index_containing(df, "Háº NG Má»¤C") + 1                     #  TÃ¬m tá»« háº¡ng má»¥c trong sheet
                pct_start_idx = find_comparison_table_start(df)                                             #  TÃ¬m báº£ng Ä‘á»ƒ so sÃ¡nh

                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"raw_start_idx: {raw_start_idx}\n")
                #     log_file.write(f"pct_start_idx: {pct_start_idx}\n")

                # Slice the first half of the file
                df_raw = df.iloc[:pct_start_idx]                # Cáº¯t pháº§n Ä‘áº§u cá»§a file Ä‘áº¿n ngay trÆ°á»›c báº£ng so sÃ¡nh, Ä‘Æ°á»£c coi lÃ  dá»¯ liá»‡u thÃ´.
                df_raw = df_raw.dropna(axis=1, how='all')       # Bá» cÃ¡c cá»™t toÃ n giÃ¡ trá»‹ NaN (trá»‘ng hoÃ n toÃ n).
                min_valid_cells = 10                            # Chá»‰ giá»¯ láº¡i cÃ¡c cá»™t cÃ³ Ã­t nháº¥t 10 Ã´ khÃ´ng trá»‘ng.
                df_raw = df_raw.loc[:, df_raw.notna().sum() >= min_valid_cells].reset_index(drop=True)

                non_empty_cols_raw = df_raw.columns[df_raw.notna().any()].tolist()          # TÃ¬m vá»‹ trÃ­ cá»™t Ä‘áº§u tiÃªn cÃ³ dá»¯ liá»‡u thá»±c, sau Ä‘Ã³ raw_col_start lÃ  cá»™t tiáº¿p theo â€” thÆ°á»ng dÃ¹ng Ä‘á»ƒ báº¯t Ä‘áº§u Ä‘á»c pháº§n giÃ¡ trá»‹ hoáº·c thÃ´ng sá»‘.
                first_valid_raw_col_idx = df_raw.columns.get_loc(non_empty_cols_raw[0])

                raw_col_start = first_valid_raw_col_idx + 1  # TSTDG usually comes after attributes



                # Slice the second half of the file
                df_pct = df.iloc[pct_start_idx:]                # Cáº¯t pháº§n cÃ²n láº¡i tá»« vá»‹ trÃ­ pct_start_idx trá»Ÿ Ä‘i Ä‘á»ƒ láº¥y báº£ng so sÃ¡nh.
                df_pct = df_pct.dropna(axis=1, how='all')       # Bá» cá»™t trá»‘ng hoÃ n toÃ n.
                min_valid_cells = 5
                df_pct = df_pct.loc[:, df_pct.notna().sum() >= min_valid_cells].reset_index(drop=True)          # Giá»¯ láº¡i cÃ¡c cá»™t cÃ³ Ã­t nháº¥t 5 Ã´ cÃ³ dá»¯ liá»‡u.

                non_empty_cols_cmp = df_pct.columns[df_pct.notna().any()].tolist()          # XÃ¡c Ä‘á»‹nh cá»™t báº¯t Ä‘áº§u cá»§a dá»¯ liá»‡u so sÃ¡nh, khÃ´ng cáº§n cá»™ng thÃªm náº¿u dá»¯ liá»‡u báº¯t Ä‘áº§u ngay tá»« Ä‘Ã³.
                first_valid_cmp_col_idx = df_pct.columns.get_loc(non_empty_cols_cmp[0])

                pct_col_start = first_valid_cmp_col_idx + 0  # attributes (e.g., "GiÃ¡ thá»‹ trÆ°á»ng") are in this column
                
                pct_start_idx = find_comparison_table_start(df_pct)         # Gá»i láº¡i hÃ m Ä‘á»ƒ Ä‘á»‹nh vá»‹ láº¡i (náº¿u cáº§n) vá»‹ trÃ­ báº£ng so sÃ¡nh trong pháº§n Ä‘Ã£ cáº¯t df_pct.


                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"raw_col_start: {raw_col_start}\n")
                #     log_file.write(f"pct_col_start: {pct_col_start}\n")

                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"df_pct:\n {df_pct}\n")                                                         # Ghi ná»™i dung báº£ng so sÃ¡nh (df_pct) vÃ o file log Ä‘á»ƒ tiá»‡n debug náº¿u cáº§n.

                # Indices of subtext in the excel file => Helper function for finding the end of the raw data table
                indicator_indices = find_meta_data(df, indicator_text="thá»i Ä‘iá»ƒm")                               # TÃ¬m cÃ¡c dÃ²ng chá»©a tá»« khÃ³a "thá»i Ä‘iá»ƒm" â€” thÆ°á»ng dÃ¹ng lÃ m dáº¥u má»‘c cuá»‘i báº£ng dá»¯ liá»‡u gá»‘c.
                if len(indicator_indices) < 2:
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write("âš ï¸ Could not find the second 'Thá»i Ä‘iá»ƒm ...' to determine raw_end_idx, switching to 'STT'\n")       # Náº¿u khÃ´ng Ä‘á»§ dá»¯ kiá»‡n Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘iá»ƒm káº¿t thÃºc, chuyá»ƒn sang tÃ¬m "stt" lÃ m chá»‰ má»¥c thay tháº¿.
                        log_file.write(f"indicator_indices: {indicator_indices}\n")
                    indicator_indices = find_meta_data(df, indicator_text="stt")
                    # raise ValueError("âš ï¸ Could not find the second 'Thá»i Ä‘iá»ƒm ...' to determine raw_end_idx")

                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"indicator_indices: {indicator_indices}\n")

                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"indicator_indices[1]: {indicator_indices[1]}\n")
                #     log_file.write(f"find_raw_table_end: {find_raw_table_end(df, second_eval_row=indicator_indices[1])}\n")
                    # log_file.write(f"df_pct index: {df_pct.index}\n")
                #     log_file.write(f"find_comparison_table_end: {find_comparison_table_end(df_pct)}\n")
                    
                raw_end_idx = find_raw_table_end(df, second_eval_row=indicator_indices[1])              # raw_end_idx: Vá»‹ trÃ­ dÃ²ng cuá»‘i cÃ¹ng cá»§a báº£ng dá»¯ liá»‡u gá»‘c (df_raw)
                pct_end_idx = find_comparison_table_end(df_pct)                                        # pct_end_idx: Vá»‹ trÃ­ káº¿t thÃºc báº£ng so sÃ¡nh (df_pct)


                raw_col_end = raw_col_start + raw_col_length
                pct_col_end = pct_col_start + pct_col_length                                # TÃ­nh chá»‰ sá»‘ cá»™t káº¿t thÃºc dá»±a trÃªn sá»‘ lÆ°á»£ng cá»™t Ä‘Ã£ biáº¿t (raw_col_length, pct_col_length cÃ³ thá»ƒ lÃ  biáº¿n cáº¥u hÃ¬nh).


                # ---- FIND AND PARSE TABLES FROM EXCEL FILES ----

                # Extract the raw table
                raw_table = df_raw.iloc[raw_start_idx:raw_end_idx+1, raw_col_start:raw_col_end].dropna(how="all")           # Cáº¯t vÃ¹ng dá»¯ liá»‡u tá»« df_raw theo dÃ²ng vÃ  cá»™t xÃ¡c Ä‘á»‹nh. Bá» cÃ¡c dÃ²ng trá»‘ng hoÃ n toÃ n.
                raw_table.reset_index(drop=True, inplace=True)

                # Dynamically assign column names based on actual number of columns
                num_raw_cols = raw_table.shape[1]                                                       # Kiá»ƒm tra sá»‘ cá»™t thá»±c táº¿ cÃ³ trong báº£ng.

                if num_raw_cols < 3:
                    raise ValueError(f"Expected at least 3 columns (attribute + TSTDG + at least 1 TSSS), but got {num_raw_cols}")          # YÃªu cáº§u báº£ng thÃ´ pháº£i cÃ³ Ã­t nháº¥t 3 cá»™t: "attribute", "TSTDG", vÃ  Ã­t nháº¥t má»™t "TSSS"

                raw_col_names = ["attribute", "TSTDG"] + [f"TSSS{i}" for i in range(1, num_raw_cols - 1)]               # Táº¡o tÃªn cá»™t Ä‘á»™ng: thuá»™c tÃ­nh + TSTDG + TSSS1, TSSS2, .
                raw_table.columns = raw_col_names
                raw_table['attribute'] = raw_table['attribute'].apply(normalize_att)                                # GÃ¡n tÃªn cá»™t Ä‘á»™ng: "attribute" (thuá»™c tÃ­nh), "TSTDG" (thÃ´ng sá»‘ tiÃªu chuáº©n Ä‘á»‹nh giÃ¡), "TSSS" (thÃ´ng sá»‘ so sÃ¡nh).
                                                                                                                    # Normalize tÃªn thuá»™c tÃ­nh cho chuáº©n hÃ³a so sÃ¡nh.
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"raw_table:\n {raw_table}\n") 
                    log_file.write(f" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n")



                # Bottom table has percentage comparison values (C1, C2...)
                pct_table = df_pct.iloc[pct_start_idx:pct_end_idx+1, pct_col_start:pct_col_end].dropna(how="all")           # Cáº¯t dá»¯ liá»‡u tá»« df_pct theo vÃ¹ng xÃ¡c Ä‘á»‹nh.
                pct_table.columns = ["ord", "attribute", "TSTDG", "TSSS1", "TSSS2", "TSSS3"]                                # GÃ¡n tÃªn cá»™t. ord Ä‘Æ°á»£c Ä‘iá»n giÃ¡ trá»‹ liÃªn tá»¥c náº¿u cÃ³ dÃ²ng bá»‹ trá»‘ng.
                pct_table.reset_index(drop=True, inplace=True)
                pct_table['ord'] = pct_table['ord'].ffill()
                # pct_table['ord'] = pct_table['ord'].astype(str).str.strip()
                pct_table['attribute'] = pct_table['attribute'].apply(normalize_att)                                        # Chuáº©n hÃ³a tÃªn thuá»™c tÃ­nh.
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"pct_table:\n {pct_table}\n")
                    log_file.write(f" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n")
                

                # Match attributes with corresponding field names: DB fieldName -> attribute
                att_en_vn = {                                                   # Táº¡o Ã¡nh xáº¡ giá»¯a tÃªn trÆ°á»ng trong CSDL (tiáº¿ng Anh) vá»›i thuá»™c tÃ­nh gá»‘c (Ä‘Ã£ chuáº©n hÃ³a).
                    "legalStatus": normalize_att("TÃ¬nh tráº¡ng phÃ¡p lÃ½"),
                    "location": normalize_att("Vá»‹ trÃ­ "),
                    "traffic": normalize_att("Giao thÃ´ng"), 
                    "area": normalize_att("Quy mÃ´ diá»‡n tÃ­ch (mÂ²)"), 
                    "width": normalize_att("Chiá»u rá»™ng"), 
                    "height": normalize_att("Chiá»u dÃ i"), 
                    "population": normalize_att("DÃ¢n cÆ°"),
                    "shape": normalize_att("HÃ¬nh dÃ¡ng"),
                    "other": normalize_att("Yáº¿u tá»‘ khÃ¡c (náº¿u cÃ³)"),
                }


                # Match attributes with corresponding "ord" values: attribute -> ord C1, C2, ...
                skip_attrs = [normalize_att(i) for i in ["Tá»· lá»‡", "Tá»· lá»‡ Ä‘iá»u chá»‰nh", "Má»©c Ä‘iá»u chá»‰nh"]]            # Loáº¡i bá» nhá»¯ng thuá»™c tÃ­nh khÃ´ng cáº§n Ã¡nh xáº¡ (vÃ­ dá»¥ nhÆ° tá»· lá»‡).
                main_rows = pct_table[~pct_table["attribute"].isin(skip_attrs)]                                     # Lá»c ra cÃ¡c dÃ²ng trong pct_table cÃ³ cá»™t "attribute" khÃ´ng náº±m trong skip_attrs.
                main_rows = main_rows.drop_duplicates(subset=["ord","attribute"], keep="first")                     # Xá»­ lÃ½ dá»¯ liá»‡u bá»‹ trÃ¹ng láº·p trong báº£ng pct_table. Giá»¯ láº¡i dÃ²ng Ä‘áº§u tiÃªn cho má»—i cáº·p (ord, attribute). TrÃ¡nh viá»‡c má»™t thuá»™c tÃ­nh Ä‘Æ°á»£c Ã¡nh xáº¡ nhiá»u láº§n sang cÃ¹ng má»™t ord.
                main_rows['attribute'] = main_rows['attribute'].apply(normalize_att)                                # Ãp dá»¥ng chuáº©n hÃ³a cho tá»«ng giÃ¡ trá»‹ trong cá»™t "attribute" (vÃ¬ má»™t sá»‘ cÃ³ thá»ƒ chÆ°a Ä‘Æ°á»£c chuáº©n hÃ³a trÆ°á»›c Ä‘Ã³).
                att_to_ord = dict(zip(main_rows["attribute"], main_rows["ord"]))                                    # Má»¥c Ä‘Ã­ch: Táº¡o Ã¡nh xáº¡ tá»« thuá»™c tÃ­nh (Ä‘Ã£ chuáº©n hÃ³a) â†’ mÃ£ ord tÆ°Æ¡ng á»©ng (C1, C2, ...). VÃ­ dá»¥: "vá»‹ trÃ­" â†’ "C2", "tÃ¬nh tráº¡ng phÃ¡p lÃ½ â†’ "C1"
                print("att_to_ord:", att_to_ord)
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"att_to_ord: {att_to_ord}\n")                                               # Sau Ä‘Ã³ ghi káº¿t quáº£ Ã¡nh xáº¡ vÃ o file log.
                


                # ---- EXTRACT RAW AND COMPARISON TABLES ----
                # Function to extract the raw table
                def extract_col_raw(col_name):            # tráº£ vá» má»™t dict dáº¡ng {attribute: value} tá»« báº£ng dá»¯ liá»‡u gá»‘c.
                    return dict(zip(raw_table["attribute"], raw_table[col_name].fillna(np.nan)))        # DÃ¹ng .fillna(np.nan) Ä‘á»ƒ Ä‘áº£m báº£o giÃ¡ trá»‹ thiáº¿u Ä‘Æ°á»£c xá»­ lÃ½ nháº¥t quÃ¡n.


                # Function to extract the comparison table
                def extract_col_pct(col_name):
                    ord_key = list(zip(pct_table["ord"], pct_table["attribute"]))       # Káº¿t há»£p cáº£ ord vÃ  attribute lÃ m key â†’ {(ord, attribute): value}.
                    ord_val = pct_table[col_name].fillna(np.nan)                        # DÃ¹ng trong trÆ°á»ng há»£p cÃ¡c nhÃ³m cÃ³ thá»ƒ láº·p láº¡i cÃ¹ng má»™t attribute (nhÆ°ng khÃ¡c ord, nhÆ° C1, C2...).
                    return dict(zip(ord_key, ord_val))


                # Dynamically extract all TSSS columns
                main_raw = extract_col_raw("TSTDG")                                    # Láº¥y dá»¯ liá»‡u "TSTDG" tá»« báº£ng gá»‘c â†’ dÃ¹ng lÃ m giÃ¡ trá»‹ chÃ­nh Ä‘á»ƒ so sÃ¡nh sau nÃ y.
                ref_raws = [
                    extract_col_raw(col)
                    for col in raw_col_names
                    if col.startswith("TSSS") or col.startswith("TSCM")                # Láº¥y ra táº¥t cáº£ cÃ¡c cá»™t tham chiáº¿u (TSSS1, TSSS2, ...) â†’ lÃ  cÃ¡c bá»™ dá»¯ liá»‡u dÃ¹ng Ä‘á»ƒ so sÃ¡nh vá»›i TSTDG
                ]

                # Filter out very empty reference columns
                ref_raws = [ref for ref in ref_raws if sum(pd.notna(v) for v in ref.values()) >= 10]        # Lá»c ra cÃ¡c cá»™t tham chiáº¿u cÃ³ quÃ¡ Ã­t dá»¯ liá»‡u (Ã­t hÆ¡n 10 giÃ¡ trá»‹ khÃ´ng null) â†’ Ä‘á»ƒ trÃ¡nh lÃ m nhiá»…u dá»¯ liá»‡u khi so sÃ¡nh.


                # Dynamically extract all TSSS columns from the comparison table        # TrÃ­ch xuáº¥t dá»¯ liá»‡u so sÃ¡nh tá»« báº£ng pháº§n trÄƒm
                main_pct = extract_col_pct("TSTDG")                                     # TrÃ­ch xuáº¥t dá»¯ liá»‡u chÃ­nh tá»« báº£ng pháº§n trÄƒm Ä‘á»ƒ lÃ m chuáº©n Ä‘á»‘i chiáº¿u.
                num_pct_cols = pct_table.shape[1]
                if num_pct_cols < 3:
                    raise ValueError(f"Expected at least 3 columns (ord, attribute, TSTDG, TSSS...), got {num_pct_cols}")       # Kiá»ƒm tra báº£ng pháº§n trÄƒm pháº£i cÃ³ Ã­t nháº¥t 3 cá»™t (ord, attribute, TSTDG...).

                pct_col_names = ["ord", "attribute"] + [f"TSTDG"] + [f"TSSS{i}" for i in range(1, num_pct_cols - 2)]        # Cáº­p nháº­t tÃªn cá»™t Ä‘á»™ng theo sá»‘ lÆ°á»£ng thá»±c táº¿ (vÃ­ dá»¥ náº¿u cÃ³ 3 cá»™t TSSS â†’ táº¡o TSSS1, TSSS2, TSSS3).
                pct_table.columns = pct_col_names

                # Extract TSSS* columns dynamically from pct_table
                ref_pcts = [extract_col_pct(col) for col in pct_col_names if col.startswith("TSSS") or col.startswith("TSCM")]      # Táº¡o danh sÃ¡ch cÃ¡c dict chá»©a dá»¯ liá»‡u pháº§n trÄƒm cho tá»«ng cá»™t TSSS tá»« báº£ng pct_table.

                
                # Match the index of reference properties from comparison tables to raw tables
                # def match_idx(ref_pcts, ref_raws):
                matched_idx = []                    # lÆ°u káº¿t quáº£ Ã¡nh xáº¡ giá»¯a cÃ¡c bá»™ dá»¯ liá»‡u (ref_pct vs ref_raw).
                used_indices = set()                # trÃ¡nh Ã¡nh xáº¡ trÃ¹ng láº·p vá»›i cÃ¹ng má»™t bá»™ dá»¯ liá»‡u.

                for ref_pct in ref_pcts:            # Má»—i ref_pct tÆ°Æ¡ng á»©ng vá»›i má»™t cá»™t TSSS trong báº£ng pháº§n trÄƒm. Má»¥c tiÃªu: tÃ¬m xem nÃ³ khá»›p nháº¥t vá»›i ref_raw nÃ o (dá»±a trÃªn giÃ¡ trá»‹ Ä‘á»‹nh giÃ¡ Ä‘áº¥t).
                    pct_price = get_land_price_pct(ref_pct)         # TÃ­nh toÃ¡n giÃ¡ trá»‹ Ä‘á»‹nh giÃ¡ Ä‘áº¥t tá»« báº£ng pháº§n trÄƒm (ref_pct). CÃ³ thá»ƒ tÃ­nh theo trá»ng sá»‘, tá»· lá»‡ pháº§n trÄƒm, hoáº·c Ä‘Æ¡n giáº£n lÃ  giÃ¡ trá»‹ tá»•ng há»£p.
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"ref: ref{ref_pcts.index(ref_pct)+1}_pct, pct_price: {pct_price}\n")        # Ghi tÃªn báº£ng pháº§n trÄƒm vÃ  giÃ¡ trá»‹ Ä‘á»‹nh giÃ¡ Ä‘áº¥t tÆ°Æ¡ng á»©ng vÃ o file log (debug dá»… hÆ¡n).
                        # log_file.write(f"ref_raws: {ref_raws}\n")
                    diffs = []
                    # diffs = [
                    #     abs(get_land_price_raw(ref_raw) - pct_price)
                    #     if i not in used_indices and pd.notna(get_land_price_raw(ref_raw)) else np.inf
                    #     for i, ref_raw in enumerate(ref_raws)
                    # ]
                    for i, ref_raw in enumerate(ref_raws):                                                          # Vá»›i má»—i ref_raw (má»™t báº£ng dá»¯ liá»‡u thÃ´):
                        raw_price = get_land_price_raw(ref_raw)                                                         # TÃ­nh giÃ¡ trá»‹ Ä‘á»‹nh giÃ¡ Ä‘áº¥t cá»§a nÃ³ (raw_price)
                        with open(log_file_path, "a", encoding="utf-8") as log_file:
                            log_file.write(f"ref_raw: ref{ref_raws.index(ref_raw)+1}_raw, raw_price: {raw_price}\n")
                            log_file.write(f"ref{ref_raws.index(ref_raw)+1}_raw:\n {ref_raw}\n")
                        
                        if i not in used_indices and pd.notna(raw_price):                                               # Náº¿u chÆ°a Ä‘Æ°á»£c ghÃ©p (khÃ´ng náº±m trong used_indices) vÃ  khÃ´ng bá»‹ thiáº¿u, tÃ­nh |raw_price - pct_price|
                            diffs.append(abs(raw_price - pct_price))
                        else:
                            diffs.append(np.inf)                                                                        # NgÆ°á»£c láº¡i, náº¿u Ä‘Ã£ dÃ¹ng rá»“i hoáº·c giÃ¡ trá»‹ thiáº¿u â†’ gÃ¡n khoáº£ng cÃ¡ch vÃ´ cá»±c (np.inf) Ä‘á»ƒ loáº¡i bá».

                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"ref: ref{ref_pcts.index(ref_pct)+1}_pct, diffs: {diffs}\n")

                    best_idx = int(np.argmin(diffs))                                        #  Chá»n cáº·p khá»›p tá»‘t nháº¥t. TÃ¬m chá»‰ sá»‘ cá»§a ref_raw cÃ³ sai khÃ¡c nhá» nháº¥t so vá»›i ref_pct.
                    matched_idx.append(best_idx)                                                    # Ghi nháº­n Ã¡nh xáº¡ tá»‘t nháº¥t (best_idx).
                    used_indices.add(best_idx)                                                      # ÄÃ¡nh dáº¥u chá»‰ sá»‘ Ä‘Ã£ dÃ¹ng trong used_indices Ä‘á»ƒ khÃ´ng ghÃ©p láº¡i.
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"used_indices: {used_indices}\n")
                        log_file.write(f"best_idx: {best_idx}\n")

                for i, idx in enumerate(matched_idx):                                       # In vÃ  ghi láº¡i káº¿t quáº£ Ã¡nh xáº¡
                    print(f"ref_pcts[{i}] matched with ref_raws[{idx}]")                        # In ra Ã¡nh xáº¡ tá»« má»—i ref_pct[i] sang ref_raws[idx] tÆ°Æ¡ng á»©ng.
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"ref_pcts[{i}] matched with ref_raws[{idx}]\n")

                idx_matches = dict(enumerate(matched_idx))                                      # Táº¡o dict Ã¡nh xáº¡ chá»‰ sá»‘: {0: best_idx_1, 1: best_idx_2, ...}
                # return idx_matches


                # ---- STEP 3: BUILD DATA STRUCTURES ----

                # Function to build the assetsManagement structure
                def build_assets_management(entry):         # Táº¡o pháº§n thÃ´ng tin tÃ i sáº£n chÃ­nh tá»« má»™t entry (dÃ²ng dá»¯ liá»‡u), bao gá»“m:
                    return {
                        "geoJsonPoint": get_info_location(entry.get(normalize_att("Tá»a Ä‘á»™ vá»‹ trÃ­"))),       # ThÃ´ng tin tá»a Ä‘á»™
                        "basicAssetsInfo": {
                            "basicAddressInfo": {
                                "fullAddress": str(entry.get(normalize_att("Äá»‹a chá»‰ tÃ i sáº£n"), "")),
                            },
                            "totalPrice": smart_parse_float(entry.get(normalize_att("GiÃ¡ Ä‘áº¥t (Ä‘á»“ng/mÂ²)"))),                         # Äá»‹a chá»‰, giÃ¡ Ä‘áº¥t, má»¥c Ä‘Ã­ch sá»­ dá»¥ng, diá»‡n tÃ­ch, kÃ­ch thÆ°á»›c
                            "landUsePurposeInfo": get_info_purpose(str(entry.get(normalize_att("Má»¥c Ä‘Ã­ch sá»­ dá»¥ng Ä‘áº¥t ")))),
                            "valuationLandUsePurposeInfo": get_info_purpose(str(entry.get(normalize_att("Má»¥c Ä‘Ã­ch sá»­ dá»¥ng Ä‘áº¥t ")))),
                            "area": smart_parse_float(entry.get(normalize_att("Quy mÃ´ diá»‡n tÃ­ch (mÂ²)\n(ÄÃ£ trá»« Ä‘áº¥t thuá»™c quy hoáº¡ch lá»™ giá»›i)"))),
                            "width": smart_parse_float(entry.get(normalize_att("Chiá»u rá»™ng (m)"))),
                            "height": smart_parse_float(entry.get(normalize_att("Chiá»u dÃ i (m)"))),
                            # "percentQuality": float(entry.get(normalize_att("Cháº¥t lÆ°á»£ng cÃ²n láº¡i (%)"), 0)) if pd.notna(entry.get(normalize_att("Cháº¥t lÆ°á»£ng cÃ²n láº¡i (%)"), 0)) else np.nan,
                            "percentQuality": float(val) if pd.notna(val := entry.get(normalize_att("Cháº¥t lÆ°á»£ng cÃ²n láº¡i (%)"))) and str(val).strip() != "" else 1.0,        # CÃ¡c giÃ¡ trá»‹ liÃªn quan Ä‘áº¿n giÃ¡ trá»‹ xÃ¢y dá»±ng, thÆ°Æ¡ng lÆ°á»£ng, chuyá»ƒn Ä‘á»•i, v.v.
                            "newConstructionUnitPrice": get_info_unit_price(str(entry.get(normalize_att("ÄÆ¡n giÃ¡ xÃ¢y dá»±ng má»›i (Ä‘á»“ng/mÂ²)"), 0))),
                            "constructionValue": float(entry.get(normalize_att("GiÃ¡ trá»‹ cÃ´ng trÃ¬nh xÃ¢y dá»±ng (Ä‘á»“ng)"), 0)),
                            "sellingPrice": float(entry.get(normalize_att("GiÃ¡ rao bÃ¡n (Ä‘á»“ng)"))),
                            "negotiablePrice": parse_human_number(entry.get(normalize_att("GiÃ¡ thÆ°Æ¡ng lÆ°á»£ng (Ä‘á»“ng)"))),
                            "landConversion": parse_human_number(entry.get(normalize_att("Chi phÃ­ chuyá»ƒn má»¥c Ä‘Ã­ch sá»­ dá»¥ng Ä‘áº¥t/ ChÃªnh lá»‡ch tiá»n chuyá»ƒn má»¥c Ä‘Ã­ch sá»­ dá»¥ng Ä‘áº¥t (Ä‘á»“ng)"), 0)),
                            "landRoadBoundary": float(entry.get(normalize_att("GiÃ¡ trá»‹ pháº§n Ä‘áº¥t thuá»™c lá»™ giá»›i (Ä‘á»“ng)"), np.nan)),
                            "landValue": float(entry.get(normalize_att("GiÃ¡ trá»‹ Ä‘áº¥t (Ä‘á»“ng)"), np.nan)),
                            "landPrice": float(entry.get(normalize_att("GiÃ¡ Ä‘áº¥t (Ä‘á»“ng/mÂ²)"))),
                        },
                        
                    }
                

                # Function to build the comparison/percentage fields structure
                def build_compare_fields(entry):
                    res = {}                                # Táº¡o cÃ¡c trÆ°á»ng dÃ¹ng Ä‘á»ƒ so sÃ¡nh (adjustment fields) giá»¯a tÃ i sáº£n chÃ­nh vÃ  tÃ i sáº£n tham chiáº¿u, bao gá»“m mÃ´ táº£ + pháº§n trÄƒm Ä‘iá»u chá»‰nh.
                    for key, att in att_en_vn.items():      # Táº¡o dictionary res Ä‘á»ƒ chá»©a káº¿t quáº£ cuá»‘i cÃ¹ng. Má»—i trÆ°á»ng dá»¯ liá»‡u (vd: legalStatus, location) sáº½ lÃ  1 key trong res.
                        norm_att = normalize_att(att)       # Chuáº©n hÃ³a láº¡i tÃªn thuá»™c tÃ­nh má»™t láº§n ná»¯a Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»“ng nháº¥t khi tra cá»©u trong cÃ¡c dict nhÆ° att_to_ord.
                        if norm_att in att_to_ord:          # Kiá»ƒm tra xem tÃªn thuá»™c tÃ­nh Ä‘Ã£ chuáº©n hÃ³a cÃ³ tá»“n táº¡i trong Ã¡nh xáº¡ att_to_ord hay khÃ´ng. att_to_ord Ã¡nh xáº¡ tá»« tÃªn thuá»™c tÃ­nh chuáº©n hÃ³a sang má»™t sá»‘ thá»© tá»± (ordinal),
                            try:
                                ord_val = att_to_ord[norm_att]
                                # Add base description field
                                res[key] = {                # Tá»« entry, láº¥y giÃ¡ trá»‹ mÃ´ táº£ táº¡i vá»‹ trÃ­ (ord_val, norm_att) VÃ¬ dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trong entry lÃ  kiá»ƒu MultiIndex (tuple key), nÃªn truy cáº­p báº±ng (ord_val, norm_att)
                                    "description": str(entry.get((ord_val, norm_att), ""))
                                }
                                # Add the percentage adjustments
                                res[key].update(add_pct(entry, att))        # Gá»i hÃ m add_pct() Ä‘á»ƒ thÃªm cÃ¡c giÃ¡ trá»‹:percent: tá»· lá»‡ gá»‘c, percentAdjust: tá»· lá»‡ Ä‘iá»u chá»‰nh, valueAdjust: má»©c Ä‘iá»u chá»‰nh (giÃ¡ trá»‹ quy Ä‘á»•i)


                            except Exception as e:
                                print(f"âš ï¸ Skipping attribute {key} due to error: {e}")
                                with open(log_file_path, "a", encoding="utf-8") as log_file:
                                    log_file.write(f"âš ï¸ Skipping attribute {key} due to error: {e}\n")      # Ghi log lá»—i vÃ o file náº¿u cÃ³ lá»—i truy cáº­p entry, trÃ¡nh chÆ°Æ¡ng trÃ¬nh bá»‹ crash.
                                continue
                        else:
                            print(f"âš ï¸ Skipping attribute '{key}' because it's missing in att_to_ord")      # Ghi log cáº£nh bÃ¡o ráº±ng thuá»™c tÃ­nh nÃ y khÃ´ng thá»ƒ xá»­ lÃ½ vÃ¬ chÆ°a cÃ³ Ã¡nh xáº¡ thá»© tá»±.
                            with open(log_file_path, "a", encoding="utf-8") as log_file:
                                log_file.write(f"âš ï¸ Skipping attribute '{key}' because it's missing in att_to_ord\n")
                    return res

                # Function to add percentage values to the comparison fields
                def add_pct(entry, att):
                    # print("This is entry:", entry)
                    # print("Tá»· lá»‡", float(entry.get((att_to_ord[att], "Tá»· lá»‡"), 0)))
                    # print("Tá»· lá»‡ Ä‘iá»u chá»‰nh", float(entry.get((att_to_ord[att], "Tá»· lá»‡ Ä‘iá»u chá»‰nh"), 0)))
                    # print("Má»©c Ä‘iá»u chá»‰nh", float(entry.get((att_to_ord[att], "Má»©c Ä‘iá»u chá»‰nh"), 0)))
                    return {                                                                           # HÃ m thÃªm cÃ¡c giÃ¡ trá»‹ tá»· lá»‡ Ä‘iá»u chá»‰nh
                        "percent": float(entry.get((att_to_ord[att], normalize_att("Tá»· lá»‡")), 0)),
                        "percentAdjust": float(entry.get((att_to_ord[att], normalize_att("Tá»· lá»‡ Ä‘iá»u chá»‰nh")), 0)),
                        "valueAdjust": float(entry.get((att_to_ord[att], normalize_att("Má»©c Ä‘iá»u chá»‰nh")), 0)),
                    }

                # Function to create the assetsCompareManagement structure
                def create_assets_compare(entry_pct, is_main=False):
                    data = {}
                    if not is_main:       # if it is a reference property       # Vá»›i is_main=False: DÃ¹ng Ã¡nh xáº¡ idx_matches giá»¯a ref_pcts vÃ  ref_raws Ä‘á»ƒ ghÃ©p Ä‘Ãºng tÃ i sáº£n gá»‘c tÆ°Æ¡ng á»©ng.
                        idx_pct = ref_pcts.index(entry_pct)
                        idx_raw = idx_matches[idx_pct]
                        entry_raw = ref_raws[idx_raw]
                        data["assetsManagement"] = build_assets_management(entry_raw)
                        data.update(build_compare_fields(entry_pct))
                        data["isCompare"] = True
                    else:                 # if it is the main property
                        data["assetsManagement"] = build_assets_management(main_raw)        # Vá»›i is_main=True: Táº¡o Ä‘á»‘i tÆ°á»£ng tá»« main_raw (dá»¯ liá»‡u gá»‘c).
                        data.update(build_compare_fields(entry_pct))
                        data["isCompare"] = False
                    return data


                # Create the structures for main and reference properties
                assets_cmp_mng_main = create_assets_compare(main_pct, is_main=True)
                assets_cmp_mng_refs = []
                for ref in ref_pcts:
                    assets_cmp_mng_ref = create_assets_compare(ref, is_main=False)
                    assets_cmp_mng_refs.append(assets_cmp_mng_ref)

                # Get current UTC time
                now = datetime.now(timezone.utc)
                # Create formatted string
                created_date_str = now.strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3]  # trim to milliseconds
                timestamp_int = int(now.timestamp())

                # Create the final assetsCompareManagements structure
                assets_compare_managements = [assets_cmp_mng_main] + assets_cmp_mng_refs


                # Create the final data structure for an Excel file 
                new_data = {                    # Táº¡o cáº¥u trÃºc dá»¯ liá»‡u tá»•ng thá»ƒ new_data
                    "createdDate": created_date_str,
                    "assetsCompareManagements": assets_compare_managements,
                }
                for i in range(4):
                    if new_data["assetsCompareManagements"][i]["assetsManagement"]["geoJsonPoint"] == None:
                        del new_data["assetsCompareManagements"][i]["assetsManagement"]["geoJsonPoint"]


                # ---- INSERT DATA INTO MONGODB ----

                client = MongoClient(MONGO_URI)
                # Use the correct database and collection
                db = client["assets-valuemind"]
                collection = db["Danh"]

                # Insert data into MongoDB
                insert_excel_data = collection.insert_one(new_data)
                insert_id = insert_excel_data.inserted_id
                # print(f"âœ… Inserted excel data with ID: {insert_id}")
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"âœ… Inserted excel data with ID: {insert_id}\n")
                    log_file.write("----------------------------------------------------------------------------------------------\n")

                    
            except Exception as e:
                error_message = traceback.format_exc()
                print(f"âŒ Failed to process sheet {sheet} in {file_path}:\n{error_message}")
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"âŒ Failed to process sheet {sheet} in {file_path}\n{error_message}\n")
                    log_file.write("----------------------------------------------------------------------------------------------\n")
                continue
        

    except Exception as e:
        error_message = traceback.format_exc()
        print(f"âŒ Failed to process {file_path}:\n{error_message}")
        with open(log_file_path, "a", encoding="utf-8") as log_file:
            log_file.write(f"âŒ Failed to process {file_path}\n{error_message}\n")
            log_file.write("----------------------------------------------------------------------------------------------\n")
        continue
    
    found += len(sheet_list)
    # with open(log_file_path, "a", encoding="utf-8") as log_file:
    #     log_file.write("----------------------------------------------------------------------------------------------\n")

# with open(log_file_path, "r", encoding="utf-8") as f:
#         processed_files = [line.strip() for line in f if line.strip()]
# print("Total number of processed files:", len(processed_files))

print("Total number of files:", len(sheet_map))
total_sheets = sum(len(sheets) for sheets in sheet_map.values())
# print(f"Total number of sheets: {total_sheets}")
print(f"Total number of sheets: {found + missing}")
with open(log_file_path, "a", encoding="utf-8") as log_file:
    log_file.write(f"Total number of sheets: {found + missing}\n")
# print("x:", x)
