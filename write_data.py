import pandas as pd
import datetime
from datetime import datetime, timezone
from pymongo import MongoClient
import uuid
from datetime import datetime
import numpy as np
import json
from write_data_utils import normalize_att, find_row_index_containing, smart_parse_float, \
        find_comparison_table_start, get_land_price_raw, get_land_price_pct, get_info_location, get_info_purpose, \
        get_info_unit_price, find_meta_data, find_comparison_table_end, find_raw_table_end, match_idx, parse_human_number
import os
import re
from pymongo import MongoClient
import traceback
import logging



# ---- CONFIGURATION ----
# EXCEL_FILE = "DV_Can Giuoc.xlsx"
# EXCEL_FILE = "DV_Le Trong Tan.xlsx"
MONGO_URI = "mongodb://dev-valuemind:W57mFPVT57lt3wU@10.10.0.42:27021/?replicaSet=rs0&directConnection=true&authSource=assets-valuemind"


# ---- GLOBAL PARAMETERS ----
sheet_idx = 0
raw_col_length = 11
pct_col_length = 6
year = 2024
month = "8"

# Read list of Excel paths
# with open(f"comparison_files_{month}_{year}.txt", "r", encoding="utf-8") as f:
#     excel_paths = [line.strip() for line in f if line.strip()]
# open(log_file_path", "w", encoding="utf-8").close()
# total_sheets = 0


# ---- LOAD EXCEL PATHS AND SHEETS ----
# comparison_file_path = f"comparison_files_{month}_{year}.txt"
# sheet_map = {}
#
# detect_folder = os.path.join(os.getcwd(), f"file_detection_{year}")
# comparison_file = os.path.join(detect_folder, comparison_file_path)
#
# with open(comparison_file, "r", encoding="utf-8") as f:
#     for line in f:
#         if ">>>" in line:
#             path, sheets = line.rstrip("\n").split(" >>> ")
#             sheet_map[path.strip()] = [s for s in sheets.split("&&")]

folder_path = r"D:/excel"
sheet_map = {}

# Qu√©t to√†n b·ªô file Excel trong th∆∞ m·ª•c
for filename in os.listdir(folder_path):
    if filename.endswith(".xlsx") or filename.endswith(".xls"):
        full_path = os.path.join(folder_path, filename)
        try:
            xls = pd.ExcelFile(full_path)
            sheet_map[full_path] = xls.sheet_names  # L·∫•y danh s√°ch sheet
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói ƒë·ªçc file {full_path}: {e}")

# ---- LOG FILE ----
log_folder = os.path.join(os.getcwd(), f"reading_logs_{year}")
os.makedirs(log_folder, exist_ok=True)


# üìÑ Create log file path inside the year-based folder
# log_file_path = f"reading_logs_{month}_{year}.txt"
log_file_path = os.path.join(log_folder, f"reading_logs_{month}_{year}.txt")
open(log_file_path, "w", encoding="utf-8").close()

found = 0
missing = 0

# ---- PROCESS EACH FILE AND SHEET ----
for file_path, sheet_list in sheet_map.items():
    try:
        if not os.path.isfile(file_path):
            print(f"‚≠ïÔ∏è File not found: {file_path}")
            missing += 1
            with open(log_file_path, "a", encoding="utf-8") as log_file:
                log_file.write(f"‚≠ïÔ∏è File not found: {file_path}\n")
                log_file.write("----------------------------------------------------------------------------------------------\n")
            continue
        
        with open(log_file_path, "a", encoding="utf-8") as log_file:
            log_file.write(f"sheet_list: {sheet_list}\n")
        xls = pd.ExcelFile(file_path)
        sheet_names = xls.sheet_names
        with open(log_file_path, "a", encoding="utf-8") as log_file:
            log_file.write(f"sheet_names: {sheet_names}\n")

        for sheet in sheet_list:
            try: 
                if sheet not in xls.sheet_names:
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"‚ùå Sheet '{sheet}' not found in {file_path}\n")
                        log_file.write("----------------------------------------------------------------------------------------------\n") 
                    continue

                # --- Insert your full parsing/writing logic here ---
                print(f"‚è≥ Processing {file_path}")
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"‚è≥ Processing sheet: {sheet}, {file_path}\n")

                # xls = pd.ExcelFile(file_path)
                # sheet = xls.sheet_names[sheet_idx]

                
                df = pd.read_excel(xls, sheet_name=sheet, header=None)
                df = df.dropna(axis=1, how='all')
                min_valid_cells = 5
                df = df.loc[:, df.notna().sum() >= min_valid_cells].reset_index(drop=True)

                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"df:\n {df}")

                

                # ---- DETECT RAW DATA TABLE AND COMPARISON TABLE ---- 
                raw_start_idx = find_row_index_containing(df, "H·∫†NG M·ª§C") + 1                     #  T√¨m t·ª´ h·∫°ng m·ª•c trong sheet
                pct_start_idx = find_comparison_table_start(df)                                             #  T√¨m b·∫£ng ƒë·ªÉ so s√°nh

                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"raw_start_idx: {raw_start_idx}\n")
                #     log_file.write(f"pct_start_idx: {pct_start_idx}\n")

                # Slice the first half of the file
                df_raw = df.iloc[:pct_start_idx]                # C·∫Øt ph·∫ßn ƒë·∫ßu c·ªßa file ƒë·∫øn ngay tr∆∞·ªõc b·∫£ng so s√°nh, ƒë∆∞·ª£c coi l√† d·ªØ li·ªáu th√¥.
                df_raw = df_raw.dropna(axis=1, how='all')       # B·ªè c√°c c·ªôt to√†n gi√° tr·ªã NaN (tr·ªëng ho√†n to√†n).
                min_valid_cells = 10                            # Ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt c√≥ √≠t nh·∫•t 10 √¥ kh√¥ng tr·ªëng.
                df_raw = df_raw.loc[:, df_raw.notna().sum() >= min_valid_cells].reset_index(drop=True)

                non_empty_cols_raw = df_raw.columns[df_raw.notna().any()].tolist()          # T√¨m v·ªã tr√≠ c·ªôt ƒë·∫ßu ti√™n c√≥ d·ªØ li·ªáu th·ª±c, sau ƒë√≥ raw_col_start l√† c·ªôt ti·∫øp theo ‚Äî th∆∞·ªùng d√πng ƒë·ªÉ b·∫Øt ƒë·∫ßu ƒë·ªçc ph·∫ßn gi√° tr·ªã ho·∫∑c th√¥ng s·ªë.
                first_valid_raw_col_idx = df_raw.columns.get_loc(non_empty_cols_raw[0])

                raw_col_start = first_valid_raw_col_idx + 1  # TSTDG usually comes after attributes



                # Slice the second half of the file
                df_pct = df.iloc[pct_start_idx:]                # C·∫Øt ph·∫ßn c√≤n l·∫°i t·ª´ v·ªã tr√≠ pct_start_idx tr·ªü ƒëi ƒë·ªÉ l·∫•y b·∫£ng so s√°nh.
                df_pct = df_pct.dropna(axis=1, how='all')       # B·ªè c·ªôt tr·ªëng ho√†n to√†n.
                min_valid_cells = 5
                df_pct = df_pct.loc[:, df_pct.notna().sum() >= min_valid_cells].reset_index(drop=True)          # Gi·ªØ l·∫°i c√°c c·ªôt c√≥ √≠t nh·∫•t 5 √¥ c√≥ d·ªØ li·ªáu.

                non_empty_cols_cmp = df_pct.columns[df_pct.notna().any()].tolist()          # X√°c ƒë·ªãnh c·ªôt b·∫Øt ƒë·∫ßu c·ªßa d·ªØ li·ªáu so s√°nh, kh√¥ng c·∫ßn c·ªông th√™m n·∫øu d·ªØ li·ªáu b·∫Øt ƒë·∫ßu ngay t·ª´ ƒë√≥.
                first_valid_cmp_col_idx = df_pct.columns.get_loc(non_empty_cols_cmp[0])

                pct_col_start = first_valid_cmp_col_idx + 0  # attributes (e.g., "Gi√° th·ªã tr∆∞·ªùng") are in this column
                
                pct_start_idx = find_comparison_table_start(df_pct)         # G·ªçi l·∫°i h√†m ƒë·ªÉ ƒë·ªãnh v·ªã l·∫°i (n·∫øu c·∫ßn) v·ªã tr√≠ b·∫£ng so s√°nh trong ph·∫ßn ƒë√£ c·∫Øt df_pct.


                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"raw_col_start: {raw_col_start}\n")
                #     log_file.write(f"pct_col_start: {pct_col_start}\n")

                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"df_pct:\n {df_pct}\n")                                                         # Ghi n·ªôi dung b·∫£ng so s√°nh (df_pct) v√†o file log ƒë·ªÉ ti·ªán debug n·∫øu c·∫ßn.

                # Indices of subtext in the excel file => Helper function for finding the end of the raw data table
                indicator_indices = find_meta_data(df, indicator_text="th·ªùi ƒëi·ªÉm")                               # T√¨m c√°c d√≤ng ch·ª©a t·ª´ kh√≥a "th·ªùi ƒëi·ªÉm" ‚Äî th∆∞·ªùng d√πng l√†m d·∫•u m·ªëc cu·ªëi b·∫£ng d·ªØ li·ªáu g·ªëc.
                if len(indicator_indices) < 2:
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write("‚ö†Ô∏è Could not find the second 'Th·ªùi ƒëi·ªÉm ...' to determine raw_end_idx, switching to 'STT'\n")       # N·∫øu kh√¥ng ƒë·ªß d·ªØ ki·ªán ƒë·ªÉ x√°c ƒë·ªãnh ƒëi·ªÉm k·∫øt th√∫c, chuy·ªÉn sang t√¨m "stt" l√†m ch·ªâ m·ª•c thay th·∫ø.
                        log_file.write(f"indicator_indices: {indicator_indices}\n")
                    indicator_indices = find_meta_data(df, indicator_text="stt")
                    # raise ValueError("‚ö†Ô∏è Could not find the second 'Th·ªùi ƒëi·ªÉm ...' to determine raw_end_idx")

                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"indicator_indices: {indicator_indices}\n")

                # with open(log_file_path, "a", encoding="utf-8") as log_file:
                #     log_file.write(f"indicator_indices[1]: {indicator_indices[1]}\n")
                #     log_file.write(f"find_raw_table_end: {find_raw_table_end(df, second_eval_row=indicator_indices[1])}\n")
                    # log_file.write(f"df_pct index: {df_pct.index}\n")
                #     log_file.write(f"find_comparison_table_end: {find_comparison_table_end(df_pct)}\n")
                    
                raw_end_idx = find_raw_table_end(df, second_eval_row=indicator_indices[1])              # raw_end_idx: V·ªã tr√≠ d√≤ng cu·ªëi c√πng c·ªßa b·∫£ng d·ªØ li·ªáu g·ªëc (df_raw)
                pct_end_idx = find_comparison_table_end(df_pct)                                        # pct_end_idx: V·ªã tr√≠ k·∫øt th√∫c b·∫£ng so s√°nh (df_pct)


                raw_col_end = raw_col_start + raw_col_length
                pct_col_end = pct_col_start + pct_col_length                                # T√≠nh ch·ªâ s·ªë c·ªôt k·∫øt th√∫c d·ª±a tr√™n s·ªë l∆∞·ª£ng c·ªôt ƒë√£ bi·∫øt (raw_col_length, pct_col_length c√≥ th·ªÉ l√† bi·∫øn c·∫•u h√¨nh).


                # ---- FIND AND PARSE TABLES FROM EXCEL FILES ----

                # Extract the raw table
                raw_table = df_raw.iloc[raw_start_idx:raw_end_idx+1, raw_col_start:raw_col_end].dropna(how="all")           # C·∫Øt v√πng d·ªØ li·ªáu t·ª´ df_raw theo d√≤ng v√† c·ªôt x√°c ƒë·ªãnh. B·ªè c√°c d√≤ng tr·ªëng ho√†n to√†n.
                raw_table.reset_index(drop=True, inplace=True)

                # Dynamically assign column names based on actual number of columns
                num_raw_cols = raw_table.shape[1]                                                       # Ki·ªÉm tra s·ªë c·ªôt th·ª±c t·∫ø c√≥ trong b·∫£ng.

                if num_raw_cols < 3:
                    raise ValueError(f"Expected at least 3 columns (attribute + TSTDG + at least 1 TSSS), but got {num_raw_cols}")          # Y√™u c·∫ßu b·∫£ng th√¥ ph·∫£i c√≥ √≠t nh·∫•t 3 c·ªôt: "attribute", "TSTDG", v√† √≠t nh·∫•t m·ªôt "TSSS"

                raw_col_names = ["attribute", "TSTDG"] + [f"TSSS{i}" for i in range(1, num_raw_cols - 1)]               # T·∫°o t√™n c·ªôt ƒë·ªông: thu·ªôc t√≠nh + TSTDG + TSSS1, TSSS2, .
                raw_table.columns = raw_col_names
                raw_table['attribute'] = raw_table['attribute'].apply(normalize_att)                                # G√°n t√™n c·ªôt ƒë·ªông: "attribute" (thu·ªôc t√≠nh), "TSTDG" (th√¥ng s·ªë ti√™u chu·∫©n ƒë·ªãnh gi√°), "TSSS" (th√¥ng s·ªë so s√°nh).
                                                                                                                    # Normalize t√™n thu·ªôc t√≠nh cho chu·∫©n h√≥a so s√°nh.
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"raw_table:\n {raw_table}\n") 
                    log_file.write(f" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n")



                # Bottom table has percentage comparison values (C1, C2...)
                pct_table = df_pct.iloc[pct_start_idx:pct_end_idx+1, pct_col_start:pct_col_end].dropna(how="all")           # C·∫Øt d·ªØ li·ªáu t·ª´ df_pct theo v√πng x√°c ƒë·ªãnh.
                pct_table.columns = ["ord", "attribute", "TSTDG", "TSSS1", "TSSS2", "TSSS3"]                                # G√°n t√™n c·ªôt. ord ƒë∆∞·ª£c ƒëi·ªÅn gi√° tr·ªã li√™n t·ª•c n·∫øu c√≥ d√≤ng b·ªã tr·ªëng.
                pct_table.reset_index(drop=True, inplace=True)
                pct_table['ord'] = pct_table['ord'].ffill()
                # pct_table['ord'] = pct_table['ord'].astype(str).str.strip()
                pct_table['attribute'] = pct_table['attribute'].apply(normalize_att)                                        # Chu·∫©n h√≥a t√™n thu·ªôc t√≠nh.
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"pct_table:\n {pct_table}\n")
                    log_file.write(f" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n")
                

                # Match attributes with corresponding field names: DB fieldName -> attribute
                att_en_vn = {                                                   # T·∫°o √°nh x·∫° gi·ªØa t√™n tr∆∞·ªùng trong CSDL (ti·∫øng Anh) v·ªõi thu·ªôc t√≠nh g·ªëc (ƒë√£ chu·∫©n h√≥a).
                    "legalStatus": normalize_att("T√¨nh tr·∫°ng ph√°p l√Ω"),
                    "location": normalize_att("V·ªã tr√≠ "),
                    "traffic": normalize_att("Giao th√¥ng"), 
                    "area": normalize_att("Quy m√¥ di·ªán t√≠ch (m¬≤)"), 
                    "width": normalize_att("Chi·ªÅu r·ªông"), 
                    "height": normalize_att("Chi·ªÅu d√†i"), 
                    "population": normalize_att("D√¢n c∆∞"),
                    "shape": normalize_att("H√¨nh d√°ng"),
                    "other": normalize_att("Y·∫øu t·ªë kh√°c (n·∫øu c√≥)"),
                }


                # Match attributes with corresponding "ord" values: attribute -> ord C1, C2, ...
                skip_attrs = [normalize_att(i) for i in ["T·ª∑ l·ªá", "T·ª∑ l·ªá ƒëi·ªÅu ch·ªânh", "M·ª©c ƒëi·ªÅu ch·ªânh"]]            # Lo·∫°i b·ªè nh·ªØng thu·ªôc t√≠nh kh√¥ng c·∫ßn √°nh x·∫° (v√≠ d·ª• nh∆∞ t·ª∑ l·ªá).
                main_rows = pct_table[~pct_table["attribute"].isin(skip_attrs)]                                     # L·ªçc ra c√°c d√≤ng trong pct_table c√≥ c·ªôt "attribute" kh√¥ng n·∫±m trong skip_attrs.
                main_rows = main_rows.drop_duplicates(subset=["ord","attribute"], keep="first")                     # X·ª≠ l√Ω d·ªØ li·ªáu b·ªã tr√πng l·∫∑p trong b·∫£ng pct_table. Gi·ªØ l·∫°i d√≤ng ƒë·∫ßu ti√™n cho m·ªói c·∫∑p (ord, attribute). Tr√°nh vi·ªác m·ªôt thu·ªôc t√≠nh ƒë∆∞·ª£c √°nh x·∫° nhi·ªÅu l·∫ßn sang c√πng m·ªôt ord.
                main_rows['attribute'] = main_rows['attribute'].apply(normalize_att)                                # √Åp d·ª•ng chu·∫©n h√≥a cho t·ª´ng gi√° tr·ªã trong c·ªôt "attribute" (v√¨ m·ªôt s·ªë c√≥ th·ªÉ ch∆∞a ƒë∆∞·ª£c chu·∫©n h√≥a tr∆∞·ªõc ƒë√≥).
                att_to_ord = dict(zip(main_rows["attribute"], main_rows["ord"]))                                    # M·ª•c ƒë√≠ch: T·∫°o √°nh x·∫° t·ª´ thu·ªôc t√≠nh (ƒë√£ chu·∫©n h√≥a) ‚Üí m√£ ord t∆∞∆°ng ·ª©ng (C1, C2, ...). V√≠ d·ª•: "v·ªã tr√≠" ‚Üí "C2", "t√¨nh tr·∫°ng ph√°p l√Ω ‚Üí "C1"
                print("att_to_ord:", att_to_ord)
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"att_to_ord: {att_to_ord}\n")                                               # Sau ƒë√≥ ghi k·∫øt qu·∫£ √°nh x·∫° v√†o file log.
                


                # ---- EXTRACT RAW AND COMPARISON TABLES ----
                # Function to extract the raw table
                def extract_col_raw(col_name):            # tr·∫£ v·ªÅ m·ªôt dict d·∫°ng {attribute: value} t·ª´ b·∫£ng d·ªØ li·ªáu g·ªëc.
                    return dict(zip(raw_table["attribute"], raw_table[col_name].fillna(np.nan)))        # D√πng .fillna(np.nan) ƒë·ªÉ ƒë·∫£m b·∫£o gi√° tr·ªã thi·∫øu ƒë∆∞·ª£c x·ª≠ l√Ω nh·∫•t qu√°n.


                # Function to extract the comparison table
                def extract_col_pct(col_name):
                    ord_key = list(zip(pct_table["ord"], pct_table["attribute"]))       # K·∫øt h·ª£p c·∫£ ord v√† attribute l√†m key ‚Üí {(ord, attribute): value}.
                    ord_val = pct_table[col_name].fillna(np.nan)                        # D√πng trong tr∆∞·ªùng h·ª£p c√°c nh√≥m c√≥ th·ªÉ l·∫∑p l·∫°i c√πng m·ªôt attribute (nh∆∞ng kh√°c ord, nh∆∞ C1, C2...).
                    return dict(zip(ord_key, ord_val))


                # Dynamically extract all TSSS columns
                main_raw = extract_col_raw("TSTDG")                                    # L·∫•y d·ªØ li·ªáu "TSTDG" t·ª´ b·∫£ng g·ªëc ‚Üí d√πng l√†m gi√° tr·ªã ch√≠nh ƒë·ªÉ so s√°nh sau n√†y.
                ref_raws = [
                    extract_col_raw(col)
                    for col in raw_col_names
                    if col.startswith("TSSS") or col.startswith("TSCM")                # L·∫•y ra t·∫•t c·∫£ c√°c c·ªôt tham chi·∫øu (TSSS1, TSSS2, ...) ‚Üí l√† c√°c b·ªô d·ªØ li·ªáu d√πng ƒë·ªÉ so s√°nh v·ªõi TSTDG
                ]

                # Filter out very empty reference columns
                ref_raws = [ref for ref in ref_raws if sum(pd.notna(v) for v in ref.values()) >= 10]        # L·ªçc ra c√°c c·ªôt tham chi·∫øu c√≥ qu√° √≠t d·ªØ li·ªáu (√≠t h∆°n 10 gi√° tr·ªã kh√¥ng null) ‚Üí ƒë·ªÉ tr√°nh l√†m nhi·ªÖu d·ªØ li·ªáu khi so s√°nh.


                # Dynamically extract all TSSS columns from the comparison table        # Tr√≠ch xu·∫•t d·ªØ li·ªáu so s√°nh t·ª´ b·∫£ng ph·∫ßn trƒÉm
                main_pct = extract_col_pct("TSTDG")                                     # Tr√≠ch xu·∫•t d·ªØ li·ªáu ch√≠nh t·ª´ b·∫£ng ph·∫ßn trƒÉm ƒë·ªÉ l√†m chu·∫©n ƒë·ªëi chi·∫øu.
                num_pct_cols = pct_table.shape[1]
                if num_pct_cols < 3:
                    raise ValueError(f"Expected at least 3 columns (ord, attribute, TSTDG, TSSS...), got {num_pct_cols}")       # Ki·ªÉm tra b·∫£ng ph·∫ßn trƒÉm ph·∫£i c√≥ √≠t nh·∫•t 3 c·ªôt (ord, attribute, TSTDG...).

                pct_col_names = ["ord", "attribute"] + [f"TSTDG"] + [f"TSSS{i}" for i in range(1, num_pct_cols - 2)]        # C·∫≠p nh·∫≠t t√™n c·ªôt ƒë·ªông theo s·ªë l∆∞·ª£ng th·ª±c t·∫ø (v√≠ d·ª• n·∫øu c√≥ 3 c·ªôt TSSS ‚Üí t·∫°o TSSS1, TSSS2, TSSS3).
                pct_table.columns = pct_col_names

                # Extract TSSS* columns dynamically from pct_table
                ref_pcts = [extract_col_pct(col) for col in pct_col_names if col.startswith("TSSS") or col.startswith("TSCM")]      # T·∫°o danh s√°ch c√°c dict ch·ª©a d·ªØ li·ªáu ph·∫ßn trƒÉm cho t·ª´ng c·ªôt TSSS t·ª´ b·∫£ng pct_table.

                
                # Match the index of reference properties from comparison tables to raw tables
                # def match_idx(ref_pcts, ref_raws):
                matched_idx = []                    # l∆∞u k·∫øt qu·∫£ √°nh x·∫° gi·ªØa c√°c b·ªô d·ªØ li·ªáu (ref_pct vs ref_raw).
                used_indices = set()                # tr√°nh √°nh x·∫° tr√πng l·∫∑p v·ªõi c√πng m·ªôt b·ªô d·ªØ li·ªáu.

                for ref_pct in ref_pcts:            # M·ªói ref_pct t∆∞∆°ng ·ª©ng v·ªõi m·ªôt c·ªôt TSSS trong b·∫£ng ph·∫ßn trƒÉm. M·ª•c ti√™u: t√¨m xem n√≥ kh·ªõp nh·∫•t v·ªõi ref_raw n√†o (d·ª±a tr√™n gi√° tr·ªã ƒë·ªãnh gi√° ƒë·∫•t).
                    pct_price = get_land_price_pct(ref_pct)         # T√≠nh to√°n gi√° tr·ªã ƒë·ªãnh gi√° ƒë·∫•t t·ª´ b·∫£ng ph·∫ßn trƒÉm (ref_pct). C√≥ th·ªÉ t√≠nh theo tr·ªçng s·ªë, t·ª∑ l·ªá ph·∫ßn trƒÉm, ho·∫∑c ƒë∆°n gi·∫£n l√† gi√° tr·ªã t·ªïng h·ª£p.
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"ref: ref{ref_pcts.index(ref_pct)+1}_pct, pct_price: {pct_price}\n")        # Ghi t√™n b·∫£ng ph·∫ßn trƒÉm v√† gi√° tr·ªã ƒë·ªãnh gi√° ƒë·∫•t t∆∞∆°ng ·ª©ng v√†o file log (debug d·ªÖ h∆°n).
                        # log_file.write(f"ref_raws: {ref_raws}\n")
                    diffs = []
                    # diffs = [
                    #     abs(get_land_price_raw(ref_raw) - pct_price)
                    #     if i not in used_indices and pd.notna(get_land_price_raw(ref_raw)) else np.inf
                    #     for i, ref_raw in enumerate(ref_raws)
                    # ]
                    for i, ref_raw in enumerate(ref_raws):                                                          # V·ªõi m·ªói ref_raw (m·ªôt b·∫£ng d·ªØ li·ªáu th√¥):
                        raw_price = get_land_price_raw(ref_raw)                                                         # T√≠nh gi√° tr·ªã ƒë·ªãnh gi√° ƒë·∫•t c·ªßa n√≥ (raw_price)
                        with open(log_file_path, "a", encoding="utf-8") as log_file:
                            log_file.write(f"ref_raw: ref{ref_raws.index(ref_raw)+1}_raw, raw_price: {raw_price}\n")
                            log_file.write(f"ref{ref_raws.index(ref_raw)+1}_raw:\n {ref_raw}\n")
                        
                        if i not in used_indices and pd.notna(raw_price):                                               # N·∫øu ch∆∞a ƒë∆∞·ª£c gh√©p (kh√¥ng n·∫±m trong used_indices) v√† kh√¥ng b·ªã thi·∫øu, t√≠nh |raw_price - pct_price|
                            diffs.append(abs(raw_price - pct_price))
                        else:
                            diffs.append(np.inf)                                                                        # Ng∆∞·ª£c l·∫°i, n·∫øu ƒë√£ d√πng r·ªìi ho·∫∑c gi√° tr·ªã thi·∫øu ‚Üí g√°n kho·∫£ng c√°ch v√¥ c·ª±c (np.inf) ƒë·ªÉ lo·∫°i b·ªè.

                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"ref: ref{ref_pcts.index(ref_pct)+1}_pct, diffs: {diffs}\n")

                    best_idx = int(np.argmin(diffs))                                        #  Ch·ªçn c·∫∑p kh·ªõp t·ªët nh·∫•t. T√¨m ch·ªâ s·ªë c·ªßa ref_raw c√≥ sai kh√°c nh·ªè nh·∫•t so v·ªõi ref_pct.
                    matched_idx.append(best_idx)                                                    # Ghi nh·∫≠n √°nh x·∫° t·ªët nh·∫•t (best_idx).
                    used_indices.add(best_idx)                                                      # ƒê√°nh d·∫•u ch·ªâ s·ªë ƒë√£ d√πng trong used_indices ƒë·ªÉ kh√¥ng gh√©p l·∫°i.
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"used_indices: {used_indices}\n")
                        log_file.write(f"best_idx: {best_idx}\n")

                for i, idx in enumerate(matched_idx):                                       # In v√† ghi l·∫°i k·∫øt qu·∫£ √°nh x·∫°
                    print(f"ref_pcts[{i}] matched with ref_raws[{idx}]")                        # In ra √°nh x·∫° t·ª´ m·ªói ref_pct[i] sang ref_raws[idx] t∆∞∆°ng ·ª©ng.
                    with open(log_file_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"ref_pcts[{i}] matched with ref_raws[{idx}]\n")

                idx_matches = dict(enumerate(matched_idx))                                      # T·∫°o dict √°nh x·∫° ch·ªâ s·ªë: {0: best_idx_1, 1: best_idx_2, ...}
                # return idx_matches


                # ---- STEP 3: BUILD DATA STRUCTURES ----

                # Function to build the assetsManagement structure
                def build_assets_management(entry):         # T·∫°o ph·∫ßn th√¥ng tin t√†i s·∫£n ch√≠nh t·ª´ m·ªôt entry (d√≤ng d·ªØ li·ªáu), bao g·ªìm:
                    return {
                        "geoJsonPoint": get_info_location(entry.get(normalize_att("T·ªça ƒë·ªô v·ªã tr√≠"))),       # Th√¥ng tin t·ªça ƒë·ªô
                        "basicAssetsInfo": {
                            "basicAddressInfo": {
                                "fullAddress": str(entry.get(normalize_att("ƒê·ªãa ch·ªâ t√†i s·∫£n"), "")),
                            },
                            "totalPrice": smart_parse_float(entry.get(normalize_att("Gi√° ƒë·∫•t (ƒë·ªìng/m¬≤)"))),                         # ƒê·ªãa ch·ªâ, gi√° ƒë·∫•t, m·ª•c ƒë√≠ch s·ª≠ d·ª•ng, di·ªán t√≠ch, k√≠ch th∆∞·ªõc
                            "landUsePurposeInfo": get_info_purpose(str(entry.get(normalize_att("M·ª•c ƒë√≠ch s·ª≠ d·ª•ng ƒë·∫•t ")))),
                            "valuationLandUsePurposeInfo": get_info_purpose(str(entry.get(normalize_att("M·ª•c ƒë√≠ch s·ª≠ d·ª•ng ƒë·∫•t ")))),
                            "area": smart_parse_float(entry.get(normalize_att("Quy m√¥ di·ªán t√≠ch (m¬≤)\n(ƒê√£ tr·ª´ ƒë·∫•t thu·ªôc quy ho·∫°ch l·ªô gi·ªõi)"))),
                            "width": smart_parse_float(entry.get(normalize_att("Chi·ªÅu r·ªông (m)"))),
                            "height": smart_parse_float(entry.get(normalize_att("Chi·ªÅu d√†i (m)"))),
                            # "percentQuality": float(entry.get(normalize_att("Ch·∫•t l∆∞·ª£ng c√≤n l·∫°i (%)"), 0)) if pd.notna(entry.get(normalize_att("Ch·∫•t l∆∞·ª£ng c√≤n l·∫°i (%)"), 0)) else np.nan,
                            "percentQuality": float(val) if pd.notna(val := entry.get(normalize_att("Ch·∫•t l∆∞·ª£ng c√≤n l·∫°i (%)"))) and str(val).strip() != "" else 1.0,        # C√°c gi√° tr·ªã li√™n quan ƒë·∫øn gi√° tr·ªã x√¢y d·ª±ng, th∆∞∆°ng l∆∞·ª£ng, chuy·ªÉn ƒë·ªïi, v.v.
                            "newConstructionUnitPrice": get_info_unit_price(str(entry.get(normalize_att("ƒê∆°n gi√° x√¢y d·ª±ng m·ªõi (ƒë·ªìng/m¬≤)"), 0))),
                            "constructionValue": float(entry.get(normalize_att("Gi√° tr·ªã c√¥ng tr√¨nh x√¢y d·ª±ng (ƒë·ªìng)"), 0)),
                            "sellingPrice": float(entry.get(normalize_att("Gi√° rao b√°n (ƒë·ªìng)"))),
                            "negotiablePrice": parse_human_number(entry.get(normalize_att("Gi√° th∆∞∆°ng l∆∞·ª£ng (ƒë·ªìng)"))),
                            "landConversion": parse_human_number(entry.get(normalize_att("Chi ph√≠ chuy·ªÉn m·ª•c ƒë√≠ch s·ª≠ d·ª•ng ƒë·∫•t/ Ch√™nh l·ªách ti·ªÅn chuy·ªÉn m·ª•c ƒë√≠ch s·ª≠ d·ª•ng ƒë·∫•t (ƒë·ªìng)"), 0)),
                            "landRoadBoundary": float(entry.get(normalize_att("Gi√° tr·ªã ph·∫ßn ƒë·∫•t thu·ªôc l·ªô gi·ªõi (ƒë·ªìng)"), np.nan)),
                            "landValue": float(entry.get(normalize_att("Gi√° tr·ªã ƒë·∫•t (ƒë·ªìng)"), np.nan)),
                            "landPrice": float(entry.get(normalize_att("Gi√° ƒë·∫•t (ƒë·ªìng/m¬≤)"))),
                        },
                        
                    }
                

                # Function to build the comparison/percentage fields structure
                def build_compare_fields(entry):
                    res = {}                                # T·∫°o c√°c tr∆∞·ªùng d√πng ƒë·ªÉ so s√°nh (adjustment fields) gi·ªØa t√†i s·∫£n ch√≠nh v√† t√†i s·∫£n tham chi·∫øu, bao g·ªìm m√¥ t·∫£ + ph·∫ßn trƒÉm ƒëi·ªÅu ch·ªânh.
                    for key, att in att_en_vn.items():      # T·∫°o dictionary res ƒë·ªÉ ch·ª©a k·∫øt qu·∫£ cu·ªëi c√πng. M·ªói tr∆∞·ªùng d·ªØ li·ªáu (vd: legalStatus, location) s·∫Ω l√† 1 key trong res.
                        norm_att = normalize_att(att)       # Chu·∫©n h√≥a l·∫°i t√™n thu·ªôc t√≠nh m·ªôt l·∫ßn n·ªØa ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªìng nh·∫•t khi tra c·ª©u trong c√°c dict nh∆∞ att_to_ord.
                        if norm_att in att_to_ord:          # Ki·ªÉm tra xem t√™n thu·ªôc t√≠nh ƒë√£ chu·∫©n h√≥a c√≥ t·ªìn t·∫°i trong √°nh x·∫° att_to_ord hay kh√¥ng. att_to_ord √°nh x·∫° t·ª´ t√™n thu·ªôc t√≠nh chu·∫©n h√≥a sang m·ªôt s·ªë th·ª© t·ª± (ordinal),
                            try:
                                ord_val = att_to_ord[norm_att]
                                # Add base description field
                                res[key] = {                # T·ª´ entry, l·∫•y gi√° tr·ªã m√¥ t·∫£ t·∫°i v·ªã tr√≠ (ord_val, norm_att) V√¨ d·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u trong entry l√† ki·ªÉu MultiIndex (tuple key), n√™n truy c·∫≠p b·∫±ng (ord_val, norm_att)
                                    "description": str(entry.get((ord_val, norm_att), ""))
                                }
                                # Add the percentage adjustments
                                res[key].update(add_pct(entry, att))        # G·ªçi h√†m add_pct() ƒë·ªÉ th√™m c√°c gi√° tr·ªã:percent: t·ª∑ l·ªá g·ªëc, percentAdjust: t·ª∑ l·ªá ƒëi·ªÅu ch·ªânh, valueAdjust: m·ª©c ƒëi·ªÅu ch·ªânh (gi√° tr·ªã quy ƒë·ªïi)


                            except Exception as e:
                                print(f"‚ö†Ô∏è Skipping attribute {key} due to error: {e}")
                                with open(log_file_path, "a", encoding="utf-8") as log_file:
                                    log_file.write(f"‚ö†Ô∏è Skipping attribute {key} due to error: {e}\n")      # Ghi log l·ªói v√†o file n·∫øu c√≥ l·ªói truy c·∫≠p entry, tr√°nh ch∆∞∆°ng tr√¨nh b·ªã crash.
                                continue
                        else:
                            print(f"‚ö†Ô∏è Skipping attribute '{key}' because it's missing in att_to_ord")      # Ghi log c·∫£nh b√°o r·∫±ng thu·ªôc t√≠nh n√†y kh√¥ng th·ªÉ x·ª≠ l√Ω v√¨ ch∆∞a c√≥ √°nh x·∫° th·ª© t·ª±.
                            with open(log_file_path, "a", encoding="utf-8") as log_file:
                                log_file.write(f"‚ö†Ô∏è Skipping attribute '{key}' because it's missing in att_to_ord\n")
                    return res

                # Function to add percentage values to the comparison fields
                def add_pct(entry, att):
                    # print("This is entry:", entry)
                    # print("T·ª∑ l·ªá", float(entry.get((att_to_ord[att], "T·ª∑ l·ªá"), 0)))
                    # print("T·ª∑ l·ªá ƒëi·ªÅu ch·ªânh", float(entry.get((att_to_ord[att], "T·ª∑ l·ªá ƒëi·ªÅu ch·ªânh"), 0)))
                    # print("M·ª©c ƒëi·ªÅu ch·ªânh", float(entry.get((att_to_ord[att], "M·ª©c ƒëi·ªÅu ch·ªânh"), 0)))
                    return {                                                                           # H√†m th√™m c√°c gi√° tr·ªã t·ª∑ l·ªá ƒëi·ªÅu ch·ªânh
                        "percent": float(entry.get((att_to_ord[att], normalize_att("T·ª∑ l·ªá")), 0)),
                        "percentAdjust": float(entry.get((att_to_ord[att], normalize_att("T·ª∑ l·ªá ƒëi·ªÅu ch·ªânh")), 0)),
                        "valueAdjust": float(entry.get((att_to_ord[att], normalize_att("M·ª©c ƒëi·ªÅu ch·ªânh")), 0)),
                    }

                # Function to create the assetsCompareManagement structure
                def create_assets_compare(entry_pct, is_main=False):
                    data = {}
                    if not is_main:       # if it is a reference property       # V·ªõi is_main=False: D√πng √°nh x·∫° idx_matches gi·ªØa ref_pcts v√† ref_raws ƒë·ªÉ gh√©p ƒë√∫ng t√†i s·∫£n g·ªëc t∆∞∆°ng ·ª©ng.
                        idx_pct = ref_pcts.index(entry_pct)
                        idx_raw = idx_matches[idx_pct]
                        entry_raw = ref_raws[idx_raw]
                        data["assetsManagement"] = build_assets_management(entry_raw)
                        data.update(build_compare_fields(entry_pct))
                        data["isCompare"] = True
                    else:                 # if it is the main property
                        data["assetsManagement"] = build_assets_management(main_raw)        # V·ªõi is_main=True: T·∫°o ƒë·ªëi t∆∞·ª£ng t·ª´ main_raw (d·ªØ li·ªáu g·ªëc).
                        data.update(build_compare_fields(entry_pct))
                        data["isCompare"] = False
                    return data


                # Create the structures for main and reference properties
                assets_cmp_mng_main = create_assets_compare(main_pct, is_main=True)
                assets_cmp_mng_refs = []
                for ref in ref_pcts:
                    assets_cmp_mng_ref = create_assets_compare(ref, is_main=False)
                    assets_cmp_mng_refs.append(assets_cmp_mng_ref)

                # Get current UTC time
                now = datetime.now(timezone.utc)
                # Create formatted string
                created_date_str = now.strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3]  # trim to milliseconds
                timestamp_int = int(now.timestamp())

                # Create the final assetsCompareManagements structure
                assets_compare_managements = [assets_cmp_mng_main] + assets_cmp_mng_refs


                # Create the final data structure for an Excel file 
                new_data = {                    # T·∫°o c·∫•u tr√∫c d·ªØ li·ªáu t·ªïng th·ªÉ new_data
                    "createdDate": created_date_str,
                    "assetsCompareManagements": assets_compare_managements,
                }
                for i in range(4):
                    if new_data["assetsCompareManagements"][i]["assetsManagement"]["geoJsonPoint"] == None:
                        del new_data["assetsCompareManagements"][i]["assetsManagement"]["geoJsonPoint"]


                # ---- INSERT DATA INTO MONGODB ----

                client = MongoClient(MONGO_URI)
                # Use the correct database and collection
                db = client["assets-valuemind"]
                collection = db["Danh"]

                # Insert data into MongoDB
                insert_excel_data = collection.insert_one(new_data)
                insert_id = insert_excel_data.inserted_id
                # print(f"‚úÖ Inserted excel data with ID: {insert_id}")
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"‚úÖ Inserted excel data with ID: {insert_id}\n")
                    log_file.write("----------------------------------------------------------------------------------------------\n")

                    
            except Exception as e:
                error_message = traceback.format_exc()
                print(f"‚ùå Failed to process sheet {sheet} in {file_path}:\n{error_message}")
                with open(log_file_path, "a", encoding="utf-8") as log_file:
                    log_file.write(f"‚ùå Failed to process sheet {sheet} in {file_path}\n{error_message}\n")
                    log_file.write("----------------------------------------------------------------------------------------------\n")
                continue
        

    except Exception as e:
        error_message = traceback.format_exc()
        print(f"‚ùå Failed to process {file_path}:\n{error_message}")
        with open(log_file_path, "a", encoding="utf-8") as log_file:
            log_file.write(f"‚ùå Failed to process {file_path}\n{error_message}\n")
            log_file.write("----------------------------------------------------------------------------------------------\n")
        continue
    
    found += len(sheet_list)
    # with open(log_file_path, "a", encoding="utf-8") as log_file:
    #     log_file.write("----------------------------------------------------------------------------------------------\n")

# with open(log_file_path, "r", encoding="utf-8") as f:
#         processed_files = [line.strip() for line in f if line.strip()]
# print("Total number of processed files:", len(processed_files))

print("Total number of files:", len(sheet_map))
total_sheets = sum(len(sheets) for sheets in sheet_map.values())
# print(f"Total number of sheets: {total_sheets}")
print(f"Total number of sheets: {found + missing}")
with open(log_file_path, "a", encoding="utf-8") as log_file:
    log_file.write(f"Total number of sheets: {found + missing}\n")
# print("x:", x)
